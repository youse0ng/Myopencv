{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 필요한 라이브러리 갖고 오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hyssk\\anaconda3\\envs\\pytorch_project\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import torch.optim\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST 데이터셋 불러오기와 MINI_Batch를 위한 DataLoader 생성 (데이터 파이프라인)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data의 개수: 60000\n",
      "test_data의 개수: 10000\n",
      "train_data[0]: (tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0706, 0.0706, 0.0706,\n",
      "          0.4941, 0.5333, 0.6863, 0.1020, 0.6510, 1.0000, 0.9686, 0.4980,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.1176, 0.1412, 0.3686, 0.6039, 0.6667, 0.9922, 0.9922, 0.9922,\n",
      "          0.9922, 0.9922, 0.8824, 0.6745, 0.9922, 0.9490, 0.7647, 0.2510,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1922,\n",
      "          0.9333, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
      "          0.9922, 0.9843, 0.3647, 0.3216, 0.3216, 0.2196, 0.1529, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706,\n",
      "          0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765, 0.7137,\n",
      "          0.9686, 0.9451, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.3137, 0.6118, 0.4196, 0.9922, 0.9922, 0.8039, 0.0431, 0.0000,\n",
      "          0.1686, 0.6039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0549, 0.0039, 0.6039, 0.9922, 0.3529, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.5451, 0.9922, 0.7451, 0.0078, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0431, 0.7451, 0.9922, 0.2745, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.1373, 0.9451, 0.8824, 0.6275,\n",
      "          0.4235, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3176, 0.9412, 0.9922,\n",
      "          0.9922, 0.4667, 0.0980, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1765, 0.7294,\n",
      "          0.9922, 0.9922, 0.5882, 0.1059, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0627,\n",
      "          0.3647, 0.9882, 0.9922, 0.7333, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.9765, 0.9922, 0.9765, 0.2510, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1804, 0.5098,\n",
      "          0.7176, 0.9922, 0.9922, 0.8118, 0.0078, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.1529, 0.5804, 0.8980, 0.9922,\n",
      "          0.9922, 0.9922, 0.9804, 0.7137, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0941, 0.4471, 0.8667, 0.9922, 0.9922, 0.9922,\n",
      "          0.9922, 0.7882, 0.3059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0902, 0.2588, 0.8353, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765,\n",
      "          0.3176, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.6706,\n",
      "          0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.7647, 0.3137, 0.0353,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.2157, 0.6745, 0.8863, 0.9922,\n",
      "          0.9922, 0.9922, 0.9922, 0.9569, 0.5216, 0.0431, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.5333, 0.9922, 0.9922, 0.9922,\n",
      "          0.8314, 0.5294, 0.5176, 0.0627, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000]]]), 5)\n",
      "train_data[0].shape:torch.Size([1, 28, 28])\n",
      "train_data[0][0]의 자료형: <class 'torch.Tensor'>\n",
      "torch_version:1.12.0\n",
      "['_DataLoader__initialized', '_DataLoader__multiprocessing_context', '_IterableDataset_len_called', '__annotations__', '__class__', '__class_getitem__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__orig_bases__', '__parameters__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_auto_collation', '_dataset_kind', '_get_iterator', '_get_shared_seed', '_index_sampler', '_iterator', 'batch_sampler', 'batch_size', 'check_worker_number_rationality', 'collate_fn', 'dataset', 'drop_last', 'generator', 'multiprocessing_context', 'num_workers', 'persistent_workers', 'pin_memory', 'pin_memory_device', 'prefetch_factor', 'sampler', 'timeout', 'worker_init_fn']\n",
      "train_dataloader: torch.Size([32, 1, 28, 28])\n",
      "train_dataloader의 개수: 1875\n"
     ]
    }
   ],
   "source": [
    "# MNIST 데이터셋 불러오기\n",
    "train_data=datasets.MNIST('DeepLearningVision\\data',True,transforms.ToTensor(),\n",
    "                       None,True)\n",
    "\n",
    "test_data=datasets.MNIST('DeepLearningVision\\data',False,transforms.ToTensor(),\n",
    "                       None,True)\n",
    "\n",
    "# 데이터셋의 클래스 객체 저장\n",
    "classes=train_data.classes\n",
    "\n",
    "# train_data 데이터셋 정보를 확인 (dir(train_data))\n",
    "print(f'train_data의 개수: {len(train_data)}')\n",
    "print(f'test_data의 개수: {len(test_data)}')\n",
    "print(f'train_data[0]: {train_data[0]}') # 튜플 (배열,클래스넘버)\n",
    "print(f'train_data[0].shape:{train_data[0][0].shape}') # 배열의 shape\n",
    "print(f'train_data[0][0]의 자료형: {type(train_data[0][0])}') # torch.Tensor형\n",
    "\n",
    "# 데이터셋을 BATCH_SIZE (SGD:Stochastic Gradient Descent) 스토캐스택 경사하강법을 하기 위한 DataLoader 생성\n",
    "train_dataloader=DataLoader(dataset=train_data,\n",
    "                            batch_size=32,\n",
    "                            shuffle=True)\n",
    "\n",
    "test_dataloader=DataLoader(dataset=test_data,\n",
    "                           shuffle=False,\n",
    "                           batch_size=32)\n",
    "print(f'torch_version:{torch.__version__}')\n",
    "# dataloader의 기능 function 확인 (class method, attribution)\n",
    "print(dir(train_dataloader))\n",
    "print(f'train_dataloader: {next(iter(train_dataloader))[0].shape}')\n",
    "print(f'train_dataloader의 개수: {len(train_dataloader)}') # 60000 // 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 생성 및 Optimizer와 Loss Function 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train_dataloader를 학습하기 위한 딥러닝 신경망 모델 생성\n",
    "# Fully Connected Layer 2개 생성\n",
    "class MNISTModel(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_shape:int,\n",
    "                 hidden_units:int,\n",
    "                 output_shape:int):\n",
    "        super().__init__()\n",
    "        self.layer1=torch.nn.Sequential(\n",
    "                            torch.nn.Flatten(),\n",
    "                            torch.nn.Linear(in_features=input_shape,\n",
    "                                                   out_features=hidden_units),\n",
    "                            torch.nn.Linear(in_features=hidden_units,\n",
    "                                                   out_features=output_shape)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.layer1(x)\n",
    "\n",
    "# 28 * 28 사이즈의 이미지 크기이기에 픽셀 하나를 노드 하나로 취급 \n",
    "# hidden unit 512개의 노드\n",
    "# output_shape은 클래스가 10개 있으므로, 출력층 노드도 10개로 지정\n",
    "model_0=MNISTModel(input_shape=28*28,\n",
    "                   hidden_units=512,\n",
    "                   output_shape=10).cuda()\n",
    "\n",
    "# 최적화 기법 (SGD) 정의\n",
    "# 최적화할 손실함수 정의 MSELoss : Mean Squared Loss (평균 제곱 오차)\n",
    "optimizer=torch.optim.SGD(params=model_0.parameters(),lr=0.001) # model_0의 파라미터를 갱신, learning rate는 0.01 (가중치 - 0.01 * Loss에 대한 해당 가중치의 Gradient)\n",
    "loss_fn=torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# 모델 학습하는데 걸리는 시간 측정을 위한 함수\n",
    "from timeit import default_timer as timer\n",
    "def print_train_time(start:float,\n",
    "                     end: float,\n",
    "                     device:torch.device=\"cuda\"):\n",
    "  \"\"\"Prints difference between start and end time.\"\"\"\n",
    "  total_time=end-start\n",
    "  print(f\"{device}상에서 걸린 학습시간: {total_time:.3f} seconds\")\n",
    "  return total_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "           Flatten-1                  [-1, 784]               0\n",
      "            Linear-2                  [-1, 512]         401,920\n",
      "            Linear-3                   [-1, 10]           5,130\n",
      "================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 1.55\n",
      "Estimated Total Size (MB): 1.57\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(model_0,(1,28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습과 검증 Engine 구현 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc 평가\n",
    "def accuracy_fn(prediction,true):\n",
    "    correct=torch.eq(prediction,true).sum().item()\n",
    "    acc=(correct/len(true))*100\n",
    "    return acc\n",
    "\n",
    "start_time=timer()\n",
    "train_acc_list=[]\n",
    "train_loss_list=[]\n",
    "test_loss_list=[]\n",
    "test_acc_list=[]\n",
    "EPOCH=15 # 60000개의 데이터에 대한 훑고감 정도? 10번 시행\n",
    "for epoch in tqdm(range(EPOCH)):\n",
    "    train_loss=0\n",
    "    train_acc=0\n",
    "    for Batch,(x_train,y_train) in enumerate(train_dataloader):\n",
    "        model_0.train(True)\n",
    "        # Do forward pass (로짓 계산)\n",
    "        y_logits=model_0(x_train)\n",
    "        # Calculate loss (손실 계산)\n",
    "        loss=loss_fn(y_logits,y_train)\n",
    "        train_loss+=loss\n",
    "        # acc 계산\n",
    "        train_acc+=accuracy_fn(y_logits.argmax(dim=1),y_train)\n",
    "        # optimizer zero grad (경사 추적 x)\n",
    "        optimizer.zero_grad()\n",
    "        # loss backward (역전파를 통한 gradient 구하기)\n",
    "        loss.backward()\n",
    "        # optimizer step (가중치 갱신)\n",
    "        optimizer.step()\n",
    "    train_loss/=len(train_dataloader)\n",
    "    train_acc/=len(train_dataloader)\n",
    "    train_loss_list.append(train_loss.detach().numpy())\n",
    "    train_acc_list.append(train_acc)\n",
    "    print(f' train_loss: {train_loss}')\n",
    "    # Evaluation\n",
    "    test_loss=0.0\n",
    "    test_acc=0.0\n",
    "    model_0.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X_test,y_test in test_dataloader:\n",
    "            # Forward Pass\n",
    "            test_logits=model_0(X_test)\n",
    "            # 손실 계산\n",
    "            test_loss+=loss_fn(test_logits,y_test)\n",
    "        test_acc+=accuracy_fn(test_logits.argmax(dim=1),y_test)\n",
    "        test_loss /= len(train_dataloader)\n",
    "        test_acc_list.append(test_acc)\n",
    "        test_loss_list.append(test_loss)\n",
    "    print(f\"Train_Loss: {train_loss:.2f}, Train_acc: {train_acc:.2f}, Test_Loss: {test_loss:.2f}, Test_Acc: {test_acc:.2f}\")\n",
    "end_time=timer()\n",
    "print_train_time(start_time,end_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 성능 탐구"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 플로팅 하고싶다.\n",
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "from matplotlib import font_manager, rc\n",
    "font_path = \"c:\\WINDOWS\\Fonts\\GULIM.TTC\"\n",
    "font = font_manager.FontProperties(fname=font_path).get_name()\n",
    "rc('font', family=font)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.rc('font', size=12) \n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(np.arange(EPOCH),train_acc_list,label='train_acc',color='red')\n",
    "plt.plot(np.arange(EPOCH),test_acc_list,label='test_acc',color='blue')\n",
    "plt.xlabel('EPOCHS')\n",
    "plt.title('정확성')\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(fontsize=10)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(np.arange(EPOCH),test_loss_list,label='test_loss',color='red')\n",
    "plt.plot(np.arange(EPOCH),train_loss_list,label='train_loss',color='blue')\n",
    "plt.xlabel('EPOCHS')\n",
    "plt.title('손실')\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 파라미터 Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'모델 파라미터 형태 파악: \\n {model_0.state_dict()} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'모델 파라미터 layer1.1의 가중치:\\n ',model_0.state_dict()['layer1.1.weight'])\n",
    "print(f'가중치의 길이: ',len(model_0.state_dict()['layer1.1.weight'])) # 히든 레이어의 hidden unit이 512개 ( 512개의 노드 )\n",
    "print(f'모델의 bias:\\n ',model_0.state_dict()['layer1.1.bias'])\n",
    "print('bias의 길이: ',len(model_0.state_dict()['layer1.1.bias'])) # 512개\n",
    "print('bias의 길이: ',len(model_0.state_dict()['layer1.2.bias'])) # 출력 10개"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 파라미터들이 93.5 퍼센트 정확도를 가지고 MNIST (숫자를 분류할 수 있다.) \n",
    "\n",
    "다음 모델은\n",
    "\n",
    "512개의 hidden units을 지닌 1개의 히든 레이어로 구성되어 있으며\n",
    "\n",
    "출력은 10개 (0,1,2,3,4,5,6,7,8,9)를 분류하는 분류기를 지닌다.\n",
    "\n",
    "우리가 흔히 얘기하는 다중 퍼셉트론, 딥러닝의 형태의 모델은 아니지만, 성능이 준수하게 나왔으며,\n",
    "\n",
    "흔히 사용하는 ReLU 비선형 함수를 포함시키지 않아도 분류를 잘하는 모습을 보인다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path  # 파일 경로 작성하기 다루기에 대한 python 모듈\n",
    "\n",
    "# 1. Create models directory (모델 디렉토리 생성) 수작업으로 이제 폴더를 생성을 할 수 있지만 코딩으로도 된다.\n",
    "MODEL_PATH = Path(\"models\")\n",
    "MODEL_PATH.mkdir(parents=True,exist_ok=True)\n",
    "\n",
    "# 2. Create model save path\n",
    "MODEL_NAME=\"model_0.pth\" #모델 저장은 pt, pth 형태로 저장\n",
    "MODEL_SAVE_PATH=MODEL_PATH / MODEL_NAME\n",
    "MODEL_SAVE_PATH\n",
    "\n",
    "# 3. Save the model state dict\n",
    "print(f\"Saving model to : {MODEL_SAVE_PATH}\")\n",
    "torch.save(obj=model_0.state_dict(),\n",
    "           f=MODEL_SAVE_PATH)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 Load 해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model_0 = MNISTModel(input_shape=28*28,hidden_units=512,output_shape=10)\n",
    "\n",
    "print(loaded_model_0.state_dict()) # 새로 랜덤하게 생성되는 Model.state_dict() 파라미터들"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "저장되어 있는 Model_0.pth를 새로운 loaded_model_0에게 넣어준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model_0.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
    "print(loaded_model_0.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 새롭게 load된 모델은 숫자3.jpg를 잘 인식할 수 있을까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL.Image import open\n",
    "from torchvision.io import read_image \n",
    "from torchvision.transforms import Resize,Grayscale,Compose\n",
    "testing_image=read_image(r'C:\\Users\\hyssk\\Myopencv\\DeepLearningVision\\MNIST\\숫자3.jpg')\n",
    "transform=Compose([Resize((28,28)),Grayscale()])\n",
    "transformed_image=transform(testing_image)\n",
    "transformed_image=transformed_image/255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_number=loaded_model_0(transformed_image).argmax(dim=1) # loaded_model_0에 transformed 시킨 이미지를 입력함\n",
    "classes[predict_number] # 3-three라고 잘 인식한다.\n",
    "plt.figure()\n",
    "plt.imshow(transformed_image.permute(1,2,0),cmap='gray') # 숫자 3.jpg를 모델이 예측함.\n",
    "plt.title(f'Predict(모델이 예측한 정답: {classes[predict_number]}')\n",
    "plt.axis(False)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Opencv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
